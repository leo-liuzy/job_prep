# Speculative Edits

## Problem

Your goal is to implement "speculative edits" using pytorch and huggingface with temperature 0 (greedy sampling).

### Speculative Edits

In speculative decoding, a draft model produces draft tokens that the large model verifies. Because the larger model can verify several tokens at once, it is faster than generating tokens one at a time.
Instead of using a draft model to produce these draft tokens, we can produce the draft tokens ourselves. How? Most edits are just rewrites of a given region, with some minor modifications.
For example, consider the following prompt:

"""

````
## Instructions
Rewrite the code and add a single line comment above `const [shouldRefreshGold, setShouldRefreshGold]`...

## Code
```ts
export default function Visualization() {
  const [instanceIdInputs, setInstanceIdInputs] = createSignal<
    InstanceId[] | null
  >(null);
  const [storedInput, setStoredInput] = createSignal<string>("");
  const [datapointOptions, setDatapointOptions] = createSignal<PropsInstance[]>(
    []
  );
  const [shouldRefreshGold, setShouldRefreshGold] =
    createSignal<boolean>(false);
  const [showGold, setShowGold] = createSignal<boolean>(false);
  const [selectedGoldRequestId, setSelectedGoldRequestId] = createSignal<
    string | undefined
  >(undefined);
  const [goldInstances, setGoldInstances] = createSignal<
    {
      sessionId: string;
      email: string | undefined;
      requestId: string | undefined;
      dateAdded: Date;
      type: $Enums.CppGoldExampleType;
    }[]
  >([]);
}
````

## Rewritten code

```ts

```

You should be able to generate this code much faster than vanilla token generation
with speculative edits. Why?
The model may generate a response something like:

```
export default function Visualization() {
  const [instanceIdInputs, setInstanceIdInputs] = createSignal<
    InstanceId[] | null
  >(null);
  const [storedInput, setStoredInput] = createSignal<string>("");
  const [datapointOptions, setDatapointOptions] = createSignal<PropsInstance[]>(
    []
  );
  // This is a comment!
  const [shouldRefreshGold, setShouldRefreshGold] =
    createSignal<boolean>(false);
  const [showGold, setShowGold] = createSignal<boolean>(false);
  const [selectedGoldRequestId, setSelectedGoldRequestId] = createSignal<
    string | undefined
  >(undefined);
  const [goldInstances, setGoldInstances] = createSignal<
    {
      sessionId: string;
      email: string | undefined;
      requestId: string | undefined;
      dateAdded: Date;
      type: $Enums.CppGoldExampleType;
    }[]
  >([]);
}
```

This is very similar to the original codeblock. Speculative edits implements the following:
On the first forward pass, we will feed in the entire original code block as a speculation.
Then we start generating tokens when the model disagrees with our draft.
We will generate greedily (eg temperature=0). You do not need to handle EOS tokens.

Your task will be to implement the following two algorithms, in order:

- Normal greedy generation (called vanilla_edit)
- Speculative edits (called speculative_edit)
  These two should produce identical generations, but speculative edits should run faster.

The use of AI is not allowed for this interview, but feel free to e.g. use google to look up docs for specific functions.
The model takes about a minute to load on this machine, which is normal. Remember to use the GPU.

### API hints

If you are unfamiliar with the `transformers` api, here are some examples of how to use it:

```
# get tokens of some text as pytorch tensor (shape (1, s)) while adding a begin of sentence token:
tokens = tokenizer(text, return_tensors='pt').input_ids

# get tokens of some text as pytorch tensor (shape (1, s)) without BOS token
tokens = tokenizer(text, add_special_tokens=False, return_tensors='pt').input_ids

# forward a model on some tokens
forward_outputs = model(input_ids=tokens)

# get logits from forward outputs
logits = forward_outputs.logits # b, s, v

# move model to gpu
model = model.to('cuda')

text = tokenizer.decode(tokens) # decode tokens List[int] into text
```
